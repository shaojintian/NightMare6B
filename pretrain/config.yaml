# model 根据需要更改 
architecture: 'Cybertron'
max_seq_len : 512
hidden_dim : 512
n_layers : 8
num_attention_heads : 8
multiple_of : 32
dropout : 0.0 # for pretraining 0 is good, for finetuning try 0.1+
bias : False # do we use bias inside LayerNorm and Linear layers?
intermediate_size : 1024
vocab_size: 60000
rope_scaling_factor: 1.0
rope_beta: 10000.0
rope_scaling_type: 'dynamic'
embedding_type: 'default'  # default/Abacus
batch_size: 1
max_input_ids_length: 4000
tokenizer_name: 'Qwen/Qwen2.5-1.5B' 
data_path: './dataset/da'

# training
train:
  epochs: 1
  steps_per_epoch: 1
  learning_rate: 1e-4
  learning_rate_decay: 0.0
  warmup_steps: 1000
  weight_decay: 0.0
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-8
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  seed: 42
  fp16: False
  fp16_opt_level: 'O1'
  gradient_checkpointing: False
  gradient_checkpointing_kwargs: {}
  output_pat: "results/nightmare_1.5b"


# defaults:
#   - _self_
#   - /callbacks: [checkpoint_every_n_steps, checkpoint_monitor, learning_rate_monitor]
#   - /data: openwebtext
#   - /model: small
#   - /strategy: ddp
#   - /noise: loglinear
#   - /lr_scheduler: constant_warmup
#   - /algo: bd3lm

mode: train  # train / ppl_eval / sample_eval
diffusion: absorbing_state

seed: 42

block_size: ${model.length}

loader:
  global_batch_size: 512
  eval_global_batch_size: ${.global_batch_size}
  # Note: batch_size and eval_batch_size are **per machine**
  batch_size: ${div_up:${.global_batch_size}, ${eval:${trainer.devices} * ${trainer.num_nodes}}}
  eval_batch_size: ${div_up:${.eval_global_batch_size}, ${eval:${trainer.devices} * ${trainer.num_nodes}}}
  num_workers: ${eval:"len(__import__('os').sched_getaffinity(0))"}
  pin_memory: True

sampling:
  noise_removal: False
  num_sample_batches: 1  # Total samples: `num_gpus` * `loader.eval_batch_size` * num_sample_batches
  var_length: False
  logdir: ./samples_${algo.name}_len${model.length}_blocksize${block_size}
  nucleus_p: 1.0
  first_hitting: True # should be set to true when T >> block_size
  kv_cache: False

training:
  ema: 0.9999
  antithetic_sampling: True
  sampling_eps: 1e-3
  coeff_clip: -1.
  resample: False
  sampling_eps_min: 1e-3
  sampling_eps_max: 1.0
  from_pretrained: null
  eval_nll: True # always evaluate nll if block size is 1

eval:
  checkpoint_path: ${cwd:}/checkpoints/last.ckpt  # Used to evaluate a checkpoint after training.
  disable_ema: False
  perplexity_batch_size: 8
  compute_perplexity_on_sanity: False
  gen_ppl_eval_model_name_or_path: gpt2-large  # gpt2-large, meta-llama/Llama-2-7b-hf
  generate_samples: False

optim:
  weight_decay: 0
  lr: 3e-4
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8

trainer:
  _target_: lightning.Trainer
  accelerator: cuda
  num_nodes: 1
  devices: ${device_count:}
  accumulate_grad_batches: ${div_up:${loader.global_batch_size}, ${eval:${trainer.devices} * ${loader.batch_size} * ${trainer.num_nodes}}}
  gradient_clip_val: 1.0
  precision: 'bf16'
  num_sanity_val_steps: 2
  max_steps: 1_000_000
  log_every_n_steps: 1_000
  limit_train_batches: 1.0   # train on full dataset, can be used to toggle quick run
  limit_val_batches: 1.0     # validate on full dataset, can be used to toggle quick run
  val_check_interval: 10_000

# wandb:
#   project: BD3-LMs
#   notes: Block Denoising Discrete Diffusion Language Models
#   group: null
#   job_type: null
#   name: null
#   id: ${.name}_${seed}
#   tags:
#     - ${noise.type}
#     - ${data.train}
#     - ${data.valid}

hydra:
  run:
    dir: ./
  job:
    chdir: true

checkpointing:
  # Use custom `save_dir` if, e.g., saving to S3 bucket, otherwise leave this parameter as is
  save_dir: ${cwd:}
  # Note: `checkpoints` path should correspond to `checkpoint_every_n_steps.dirpath`
  resume_from_ckpt: true
  resume_ckpt_path: ${.save_dir}/checkpoints/last.ckpt

data:
  train: "./dataset/da"