mode: train
diffusion: absorbing_state
seed: 1
block_size: ${model.length}
loader:
  global_batch_size: 512
  eval_global_batch_size: ${.global_batch_size}
  batch_size: ${div_up:${.global_batch_size}, ${eval:${trainer.devices} * ${trainer.num_nodes}}}
  eval_batch_size: ${div_up:${.eval_global_batch_size}, ${eval:${trainer.devices}
    * ${trainer.num_nodes}}}
  num_workers: ${eval:"len(__import__('os').sched_getaffinity(0))"}
  pin_memory: true
sampling:
  noise_removal: false
  num_sample_batches: 1
  var_length: false
  logdir: ./samples_${algo.name}_len${model.length}_blocksize${block_size}
  nucleus_p: 1.0
  first_hitting: true
  kv_cache: false
training:
  ema: 0.9999
  antithetic_sampling: true
  sampling_eps: 0.001
  coeff_clip: -1.0
  resample: false
  sampling_eps_min: 0.001
  sampling_eps_max: 1.0
  from_pretrained: null
  eval_nll: true
eval:
  checkpoint_path: ${cwd:}/checkpoints/last.ckpt
  disable_ema: false
  perplexity_batch_size: 8
  compute_perplexity_on_sanity: false
  gen_ppl_eval_model_name_or_path: gpt2-large
  generate_samples: false
optim:
  weight_decay: 0
  lr: 0.0003
  beta1: 0.9
  beta2: 0.999
  eps: 1.0e-08
trainer:
  _target_: lightning.Trainer
  accelerator: cuda
  num_nodes: 1
  devices: ${device_count:}
  accumulate_grad_batches: ${div_up:${loader.global_batch_size}, ${eval:${trainer.devices}
    * ${loader.batch_size} * ${trainer.num_nodes}}}
  gradient_clip_val: 1.0
  precision: bf16
  num_sanity_val_steps: 2
  max_steps: 1000000
  log_every_n_steps: 1000
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  val_check_interval: 10000
wandb:
  project: BD3-LMs
  notes: Block Denoising Discrete Diffusion Language Models
  group: null
  job_type: null
  name: null
  id: ${.name}_${seed}
  tags:
  - ${noise.type}
  - ${data.train}
  - ${data.valid}
checkpointing:
  save_dir: ${cwd:}
  resume_from_ckpt: true
  resume_ckpt_path: ${.save_dir}/checkpoints/last.ckpt
callbacks:
  checkpoint_every_n_steps:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    save_top_k: -1
    save_last: true
    dirpath: ${checkpointing.save_dir}/checkpoints
    verbose: true
    auto_insert_metric_name: false
    every_n_train_steps: 500
  checkpoint_monitor:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: val/nll
    mode: min
    save_top_k: 1
    save_last: false
    dirpath: ${checkpointing.save_dir}/checkpoints
    filename: best
    auto_insert_metric_name: false
    verbose: true
  learning_rate_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step
data:
  train: openwebtext
  valid: wikitext103
  tokenizer_name_or_path: gpt2
  cache_dir: /share/kuleshov/ssahoo/textdiffusion/data
  wrap: true
  streaming: false
model:
  name: small
  type: ddit
  hidden_size: 768
  cond_dim: 128
  length: 1024
  n_blocks: 12
  n_heads: 12
  scale_by_sigma: true
  dropout: 0.1
  tie_word_embeddings: true
  adaln: false
  attn_backend: flash_attn
strategy:
  _target_: lightning.pytorch.strategies.DDPStrategy
  find_unused_parameters: false
noise:
  type: loglinear
  sigma_min: 0.0001
  sigma_max: 20
lr_scheduler:
  _target_: transformers.get_constant_schedule_with_warmup
  num_warmup_steps: 2500
algo:
  name: bd3lm
  backbone: dit
  parameterization: subs
  time_conditioning: false
  T: 0
  causal_attention: false
  dropout: 0.0
  ignore_bos: true
  cross_attn: true
  var_min: true
  clip_search_delta: 0.05
  clip_search_widths: []
  fix_clipping: false
  sampler: semi_ar
  mdlm_loss_scale: false
